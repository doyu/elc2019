#+TITLE: [[https://sched.co/TLCJ][TinyML as-a-Service:]] Backend Porting Guide
#+AUTHOR: [[hiroshi.doyu@ericsson.com][Hiroshi Doyu]] <hiroshi.doyu@ericsson.com>
#+EMAIL: hiroshi.doyu@ericsson.com

* Design
TinyML as-a-Service (TinyMLaaS) would demonstrate ML inference orchestration
on IoT devices.
TinyMLaaS is just a thin REST API server, based on [[https://swagger.io/tools/swagger-ui/][Swagger UI]],
which backend would run a requested docker image
to build an device OS image on server. Conceptually it's similar to [[https://www.openfaas.com/][OpenFaaS]].
Each API call is equivalent to running a docker image on server.
Those docker images would compile ML inference and then generate an device OS image
which also includes ML inference in it.
Those OS build docker images are expected to be provided
by 3rd party partners (AI chip/ML compiler vendors).
The installation of those generaeted device OS images is done
via [[https://www.omaspecworks.org/what-is-oma-specworks/iot/lightweight-m2m-lwm2m/][OMA LwM2M]] [[https://foundries.io/insights/2018/01/17/20180117-lwm2m-update/][FOTA]] (Firmware Other The Air) update
with an URL pointing to an OS image stored on TinyMLaaS,
as seen in Figure 1.

#+CAPTION: TinyMLaaS ecosystem
[[./images/ecosystem_017.png]]


This document explains
how TinyMLaaS is to be developed independently by different entities.
TinyMLaaS includes 4 components:

1. Web App
2. API server (REST)
3. device OS builder (+ML compiler)
4. device OS updater


* Web App
This is a Frontend UI for users.
Once API servier (backend) is implemented,
both Web App (+Mobile App?) could be implemented just by calling those REST API.
Web App could provide nice UI for demonstration, but this could be done later.
since all REST API can be called via[[https://swagger.io/tools/swagger-ui/][Swagger UI]].
Test could be done via [[https://swagger.io/tools/swagger-ui/][Swagger UI]] without Web App.

* API server
TinyMLaaS is a REST API server. This server side stub is
automatically generated by [[https://www.openapis.org/][Open API Specification]] (OAS) file,
hosted by [[https://swagger.io/tools/swaggerhub/][Swagger HUB]]. This also could generate client side code too.
The backend of API server would simply run the specified docker image,
stored in [[https://hub.docker.com/][DockerHUB]]. This docker image accepts some parameteres passed
via REST. For example, target device type and inference model name.
Probably Go would be the generated stub
since it may be easier to call docker API later
to run a docker image.

#+CAPTION: API server
[[./images/arch.png]]


* device OS builder
OS build procedure is packed in a docker image.
If you pull and run this docker image, it would generate a new OS image,
which include a specified inference model.
Those docker images are provided by partner companies.
Those docker images are stored in [[https://hub.docker.com/][DockerHUB]].
We'll support a limitted number of target boards.
OS image should include LwM2M client and FOTA.
A generated OS images are stored in TinyMLaaS.
It could be downloaded by http.

#+CAPTION: device OS builder
[[./images/arch_001.png]]


* device OS updator
LwM2M FOTA would replace a whole OS image with updated interence model.
Leshan server(LwM2M server) has pulished a REST API for FOTA.
A client would trigger FOTA via the above Leshan REST API for FOTA.
There's no implementation work here.

#+CAPTION: device OS updator
[[./images/arch_002.png]]

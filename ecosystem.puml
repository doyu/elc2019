@startuml

title Traditionally

cloud cloud {
        cloud AWS
        cloud Azure
        cloud GCP
}

node device1
node device2

AWS <---- device1 : request
AWS ----> device1 : return
AWS <---- device2 : request
AWS ----> device2 : return

newpage
title Currently (1/2)

cloud cloud {
        cloud AWS
        cloud Azure
        cloud GCP
}
cloud edge
node device1
node device2

AWS <-down- edge : request
AWS -down-> edge : return

edge <- device1 : request
edge -down-> device1 : return
edge <- device2 : request
edge -down-> device2 : return

newpage
title Currently (2/2)

cloud cloud {
        cloud AWS
        cloud Azure
        cloud GCP
}
cloud edge

note left of edge
        Privacy
        Bandwidth
        Latency
        Reliability
end note

rectangle on_prem {
        collections devices
        database battery
        database privacy
}

note right of devices : Energy

AWS <-down- edge : request
AWS -down-> edge : return

edge <.down. devices : request
edge .down.> devices : return

devices -down-> privacy : request
devices <- privacy : data
battery .up.> devices : power



newpage
title Future

cloud cloud {
        cloud AWS
        cloud Azure
        cloud GCP
}
cloud edge
'note right of edge
'computing expanding
'more downwards
'end note

rectangle on_prem {
        node device1
        node device2
        node device3
}

'note right of on_prem
'        more in-place execution
'        collaborate if not in-place
'        more on-demand
'end note

AWS <-down- edge : request
AWS -down-> edge : return

edge <.down.> device1
edge <.down.> device2

device1 <-> device2
device1 <-down-> device3
device2 <-down-> device3 : offloading
device3 <-> device3 : in place


newpage
title Web experience

cloud "Web/linux\n" #5DADE2 {
        cloud cloud {
                cloud AWS
                cloud Azure
                cloud GCP
        }
        cloud edge
}

note right of cloud
Linux based
Container based microservices
on-demand service orchestration
end note

rectangle on-premises {
        node device1
        node device2
        node device3
}

AWS <-down- edge : request
AWS -> edge : return

edge <.down.> device1
edge <.down.> device2

device1 <-> device2

device1 <-down-> device3
device2 <-down-> device3

newpage
title Embedded experience

cloud "Web/linux\n" {
        cloud cloud {
                cloud AWS
                cloud Azure
                cloud GCP
        }
        cloud edge
}

AWS <-down- edge : request
AWS -> edge : return

rectangle "on_prem/\nEmbedded" #F39C12 {
        node device1
        node device2
        node device3
}

edge <.down.> device1
edge <.down.> device2
device1 <-> device2
device1 <-down-> device3
device2 <-down-> device3 : offloading
device3 <-> device3 : in place

note right of device2
Linux may not be too big?
variety of RTOSes
Fixed services
no orchestration
end note

newpage
title Gap between Web and Embedded

cloud "Web/linux\n" #5DADE2 {
        cloud cloud {
                cloud AWS
                cloud Azure
                cloud GCP
        }
        cloud edge
}

note right of cloud
Linux based
Container based microservices
on-demand service orchestration
end note

rectangle Embedded #F39C12 {
        node device1
        node device2
        node device3
}

note right of Embedded
Linux may not be too big?
variety of RTOSes
Fixed services
no orchestration
end note

AWS <-down- edge : request
AWS -> edge : return

edge <.down.> device1
edge <.down.> device2
device1 <-right-> device2
device1 <-down-> device3
device2 <-down-> device3 : offloading
device3 <-> device3 : in place

newpage
title ML environment

cloud "Web/linux\n" #5DADE2 {
        cloud cloud {
                cloud AWS
                cloud Azure
                cloud GCP
        }
        cloud edge
}

note right of cloud
variety of python ML frameworks
heavy training & inference
multi-node GPUs
end note

note right of edge
Infrence is OK,
but training?
end note

rectangle Embedded #F39C12 {
        node device1
        node device2
        node device3
}

note right of Embedded
<s>python ML runtime</s>
<s>training</s>
Any inference in place?
end note

AWS <-down- edge : request
AWS -> edge : return

edge <.down.> device1
edge <.down.> device2
device1 <-right-> device2
device1 <-down-> device3
device2 <-down-> device3 : offloading
device3 <-> device3 : in place

newpage
title Current

cloud "Web/linux\n" #5DADE2 {
        cloud cloud {
                cloud AWS
                cloud Azure
                cloud GCP
        }
        cloud edge
}

rectangle Embedded #F39C12 {
        node device1
        node device2
        node device3
}

AWS <-down- edge : request
AWS -> edge : return

edge <.down.> device1
edge <.down.> device2
device1 <-right-> device2
device1 <-down-> device3
device2 <-down-> device3 : offloading
device3 <-> device3 : in place

newpage
title Proposal: TinyML as-a-Service

cloud "Web/linux\n" #5DADE2 {
        cloud cloud {
                cloud AWS
                cloud Azure
                cloud GCP
        }
        cloud edge
}

database Zoo {
        collections "model"
}

cloud "TinyMLaaS\n" #F012F3 {
        folder SaaS {
                (Build\nService) as TBS
        }
}

TBS <-up- Zoo : downloading

rectangle Embedded #F39C12 {
        node device1
        node device2
        node device3
}


file "image\nruntime" as download

TBS -down- download : downloading
device2 -up-> SaaS : info
device2 <-up- download

AWS <-down- edge : request
AWS -> edge : return

edge <.down.> device1
edge <.down.> device2
device1 <-right-> device2
device1 <-down-> device3
device2 <-down-> device3 : offloading
device3 <-> device3 : in place
device2 <-> device2 : ML in place

@enduml
